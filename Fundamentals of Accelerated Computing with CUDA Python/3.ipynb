{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def tile_transpose_conflict_free(a, transposed):\n",
    "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
    "    # and that `a` is a multiple of these dimensions.\n",
    "    \n",
    "    # 1) Create 32x32 shared memory array.\n",
    "    tile = cuda.shared.array((32, 33), numba_types.int32)                  # <+++++++++++\n",
    "\n",
    "    # Compute offsets into global input array.\n",
    "#     x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x             # <------------\n",
    "#     y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y             # <------------\n",
    "    x, y = cuda.grid(2)                                                    # <+++++++++++\n",
    "    \n",
    "    # 2) Make coalesced read from global memory into shared memory array.\n",
    "    # Note the use of local thread indices for the shared memory write,\n",
    "    # and global offsets for global memory read.\n",
    "    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[y, x]\n",
    "\n",
    "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # 4) Calculate transposed location for the shared memory array tile\n",
    "    # to be written back to global memory.\n",
    "    t_x = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
    "    t_y = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
    "\n",
    "    # 5) Write back to global memory,\n",
    "    # transposing each element within the shared memory array.\n",
    "    transposed[t_y, t_x] = tile[cuda.threadIdx.x, cuda.threadIdx.y]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
